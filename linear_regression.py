import torch 
import numpy as np
import matplotlib.pyplot as plt

x_data = torch.tensor([[1.], [2.], [3.]])
y_data = torch.tensor([[2.], [4.], [6.]])
"""
nn.module must use matrix of size n*1 

x_data = torch.tensor([1.])
y_data = torch.tensor([2.])
"""

# neural network -> nn
class LinearModule(torch.nn.Module):
    def __init__(self):
        super(LinearModule, self).__init__()
        # Build a model of y=wx+b
        # The size of x is in_features*1
        # The size of y is out_features*1
        self.linear = torch.nn.Linear(in_features = 1, out_features = 1, bias = True)
        
    def forward(self, x):
        # Generate a tensor which size is same as x_data 
        # I guess pred_y's generated by class self.linear.__call__()
        pred_y = self.linear(x)
        # print(f'forward()->pred_y = {pred_y}')
        return pred_y
    
episode_l = []
w_l = []

def dl():
    model = LinearModule()
    criterion = torch.nn.MSELoss(reduction = 'sum')
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
    for episode in np.arange(1, 101, 1):
        # model(x_data) call function __call__() by default and then __call__() function can activate forward()
        # In other words, pred_y <==> forward()->pred_y 
        pred_y = model(x_data)
        # print(pred_y)
        loss = criterion(pred_y, y_data)
        # print(f'loss = {loss}\ntype(loss) = {type(loss)}\n')
        
        # loss calculate the result, so loss need to process backward
        # loss is a tensor and a scalar
        loss.backward()
        """
        backward() generate grads for every tensor
        optimizer get a grad, too
        So optimizer use step() function update the parameters of weight and bias
        """
        optimizer.step()
        optimizer.zero_grad()
        
        episode_l.append(episode)
        w_l.append(model.linear.weight.item())
        # ---------model.linear.bias.item()
    
        
def show():
    plt.xlabel('epoch')
    plt.ylabel('w')
    plt.plot(episode_l, w_l)
    plt.show()
    
    
if __name__ == "__main__":
    dl()
    show()